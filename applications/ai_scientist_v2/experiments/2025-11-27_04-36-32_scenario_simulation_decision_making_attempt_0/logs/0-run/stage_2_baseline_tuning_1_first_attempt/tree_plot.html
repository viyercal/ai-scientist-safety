<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 2], [0, 1], [1, 3]], "layout": [[0.5, 0.0], [0.0, 0.5], [1.0, 0.5], [0.0, 1.0]], "plan": ["To implement the basic working solution for integrating scenario simulation with\nreinforcement learning using language models (LLMs), we can start by simulating\na simple environment where an agent needs to make decisions based on generated\nscenarios. We will create synthetic data to represent the current states and\noptimal actions, and we'll evaluate our model using the Scenario Evaluation\nScore (SES). The code will include a basic reinforcement learning setup, where\nthe agent takes actions based on the simulated scenarios. The implementation\nwill also ensure that the validation loss and SES are tracked and printed during\ntraining, along with saving all relevant data for later use.", "Hyperparam tuning name: learning_rate. The proposed solution involves\nimplementing hyperparameter tuning for the learning rate by iterating through a\npredefined list of learning rates, training the model with each one, and storing\nthe training losses and metrics for analysis. After training with each learning\nrate, we save all relevant data in a structured format, allowing for future\ncomparison of the different configurations.", "Hyperparam tuning name: batch_size. To implement hyperparameter tuning for the\nbatch size, I will define a list of possible batch sizes to experiment with. For\neach batch size, I will initialize the DataLoader and run the training loop,\nwhile storing the metrics and losses in the experiment_data dictionary for each\nconfiguration. After training is complete, I will save the experiment data as a\nnumpy array.", "Seed node"], "code": ["import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport random\n\n# Create working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Simple synthetic dataset\nclass SyntheticDataset(Dataset):\n    def __init__(self, size=1000):\n        self.data = torch.randn(size, 10)  # 10 features\n        self.labels = (\n            self.data.sum(axis=1) > 0\n        ).float()  # Binary classification based on sum\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n\n# Simple neural network\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 1), nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# Hyperparameters\nnum_epochs = 10\nbatch_size = 32\nlearning_rate = 0.001\n\n# Initialize dataset and dataloader\ndataset = SyntheticDataset()\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Initialize model, optimizer, and loss function\nmodel = SimpleNN().to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = nn.BCELoss()\n\n# Prepare experiment data storage\nexperiment_data = {\n    \"synthetic_dataset\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    },\n}\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    for inputs, labels in dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs).squeeze()\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    avg_train_loss = train_loss / len(dataloader)\n    experiment_data[\"synthetic_dataset\"][\"losses\"][\"train\"].append(avg_train_loss)\n    print(f\"Epoch {epoch + 1}: training_loss = {avg_train_loss:.4f}\")\n\n    # Calculate Scenario Evaluation Score (SES)\n    predictions = model(dataset.data.to(device)).cpu().detach().numpy()\n    ses = np.mean(predictions)  # Simplistic SES for example purpose\n    experiment_data[\"synthetic_dataset\"][\"metrics\"][\"train\"].append(ses)\n    print(f\"Epoch {epoch + 1}: SES = {ses:.4f}\")\n\n# Save the experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\n# Create working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Simple synthetic dataset\nclass SyntheticDataset(Dataset):\n    def __init__(self, size=1000):\n        self.data = torch.randn(size, 10)  # 10 features\n        self.labels = (\n            self.data.sum(axis=1) > 0\n        ).float()  # Binary classification based on sum\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n\n# Simple neural network\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 1), nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# Hyperparameters\nnum_epochs = 10\nbatch_size = 32\nlearning_rates = [0.0001, 0.001, 0.01]  # Different learning rates for tuning\n\n# Initialize dataset and dataloader\ndataset = SyntheticDataset()\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Prepare experiment data storage\nexperiment_data = {\n    \"hyperparam_tuning_learning_rate\": {\n        \"synthetic_dataset\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n    },\n}\n\n# Training loop for different learning rates\nfor lr in learning_rates:\n    model = SimpleNN().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.BCELoss()\n\n    print(f\"Training with learning rate: {lr}\")\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs).squeeze()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(dataloader)\n        experiment_data[\"hyperparam_tuning_learning_rate\"][\"synthetic_dataset\"][\n            \"losses\"\n        ][\"train\"].append(avg_train_loss)\n        print(f\"Epoch {epoch + 1}: training_loss = {avg_train_loss:.4f}\")\n\n        # Calculate Scenario Evaluation Score (SES)\n        predictions = model(dataset.data.to(device)).cpu().detach().numpy()\n        ses = np.mean(predictions)  # Simplistic SES for example purpose\n        experiment_data[\"hyperparam_tuning_learning_rate\"][\"synthetic_dataset\"][\n            \"metrics\"\n        ][\"train\"].append(ses)\n        print(f\"Epoch {epoch + 1}: SES = {ses:.4f}\")\n\n# Save the experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\n# Create working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Simple synthetic dataset\nclass SyntheticDataset(Dataset):\n    def __init__(self, size=1000):\n        self.data = torch.randn(size, 10)  # 10 features\n        self.labels = (\n            self.data.sum(axis=1) > 0\n        ).float()  # Binary classification based on sum\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n\n# Simple neural network\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 1), nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# Hyperparameters\nnum_epochs = 10\nbatch_sizes = [16, 32, 64]  # List of batch sizes to tune\nlearning_rate = 0.001\n\n# Prepare experiment data storage\nexperiment_data = {\n    \"batch_size_tuning\": {},\n}\n\n# Initialize dataset\ndataset = SyntheticDataset()\n\n# Hyperparameter tuning for batch sizes\nfor batch_size in batch_sizes:\n    print(f\"Training with batch size: {batch_size}\")\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    # Initialize model, optimizer, and loss function\n    model = SimpleNN().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    criterion = nn.BCELoss()\n\n    # Store data for the current batch size\n    experiment_data[\"batch_size_tuning\"][batch_size] = {\n        \"metrics\": {\"train\": []},\n        \"losses\": {\"train\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    # Training loop\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs).squeeze()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(dataloader)\n        experiment_data[\"batch_size_tuning\"][batch_size][\"losses\"][\"train\"].append(\n            avg_train_loss\n        )\n        print(f\"Epoch {epoch + 1}: training_loss = {avg_train_loss:.4f}\")\n\n        # Calculate Scenario Evaluation Score (SES)\n        predictions = model(dataset.data.to(device)).cpu().detach().numpy()\n        ses = np.mean(predictions)  # Simplistic SES for example purpose\n        experiment_data[\"batch_size_tuning\"][batch_size][\"metrics\"][\"train\"].append(ses)\n        print(f\"Epoch {epoch + 1}: SES = {ses:.4f}\")\n\n    # Store final predictions and ground truth\n    experiment_data[\"batch_size_tuning\"][batch_size][\"predictions\"].append(predictions)\n    experiment_data[\"batch_size_tuning\"][batch_size][\"ground_truth\"].append(\n        dataset.labels.numpy()\n    )\n\n# Save the experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\n# Create working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Simple synthetic dataset\nclass SyntheticDataset(Dataset):\n    def __init__(self, size=1000):\n        self.data = torch.randn(size, 10)  # 10 features\n        self.labels = (\n            self.data.sum(axis=1) > 0\n        ).float()  # Binary classification based on sum\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n\n# Simple neural network\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 1), nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# Hyperparameters\nnum_epochs = 10\nbatch_size = 32\nlearning_rates = [0.0001, 0.001, 0.01]  # Different learning rates for tuning\n\n# Initialize dataset and dataloader\ndataset = SyntheticDataset()\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Prepare experiment data storage\nexperiment_data = {\n    \"hyperparam_tuning_learning_rate\": {\n        \"synthetic_dataset\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n    },\n}\n\n# Training loop for different learning rates\nfor lr in learning_rates:\n    model = SimpleNN().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.BCELoss()\n\n    print(f\"Training with learning rate: {lr}\")\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs).squeeze()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(dataloader)\n        experiment_data[\"hyperparam_tuning_learning_rate\"][\"synthetic_dataset\"][\n            \"losses\"\n        ][\"train\"].append(avg_train_loss)\n        print(f\"Epoch {epoch + 1}: training_loss = {avg_train_loss:.4f}\")\n\n        # Calculate Scenario Evaluation Score (SES)\n        predictions = model(dataset.data.to(device)).cpu().detach().numpy()\n        ses = np.mean(predictions)  # Simplistic SES for example purpose\n        experiment_data[\"hyperparam_tuning_learning_rate\"][\"synthetic_dataset\"][\n            \"metrics\"\n        ][\"train\"].append(ses)\n        print(f\"Epoch {epoch + 1}: SES = {ses:.4f}\")\n\n# Save the experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n"], "term_out": ["['Using device: cuda', '\\n', 'Epoch 1: training_loss = 0.6717', '\\n', 'Epoch 1:\nSES = 0.5073', '\\n', 'Epoch 2: training_loss = 0.6226', '\\n', 'Epoch 2: SES =\n0.5117', '\\n', 'Epoch 3: training_loss = 0.5723', '\\n', 'Epoch 3: SES = 0.5140',\n'\\n', 'Epoch 4: training_loss = 0.5104', '\\n', 'Epoch 4: SES = 0.5145', '\\n',\n'Epoch 5: training_loss = 0.4463', '\\n', 'Epoch 5: SES = 0.5102', '\\n', 'Epoch\n6: training_loss = 0.3863', '\\n', 'Epoch 6: SES = 0.5065', '\\n', 'Epoch 7:\ntraining_loss = 0.3341', '\\n', 'Epoch 7: SES = 0.5040', '\\n', 'Epoch 8:\ntraining_loss = 0.2888', '\\n', 'Epoch 8: SES = 0.5032', '\\n', 'Epoch 9:\ntraining_loss = 0.2533', '\\n', 'Epoch 9: SES = 0.5059', '\\n', 'Epoch 10:\ntraining_loss = 0.2234', '\\n', 'Epoch 10: SES = 0.5060', '\\n', 'Execution time:\n6 seconds seconds (time limit is 10 minutes).']", "['Using device: cuda', '\\n', 'Training with learning rate: 0.0001', '\\n', 'Epoch\n1: training_loss = 0.6935', '\\n', 'Epoch 1: SES = 0.4966', '\\n', 'Epoch 2:\ntraining_loss = 0.6889', '\\n', 'Epoch 2: SES = 0.4979', '\\n', 'Epoch 3:\ntraining_loss = 0.6853', '\\n', 'Epoch 3: SES = 0.4995', '\\n', 'Epoch 4:\ntraining_loss = 0.6788', '\\n', 'Epoch 4: SES = 0.5010', '\\n', 'Epoch 5:\ntraining_loss = 0.6750', '\\n', 'Epoch 5: SES = 0.5022', '\\n', 'Epoch 6:\ntraining_loss = 0.6696', '\\n', 'Epoch 6: SES = 0.5033', '\\n', 'Epoch 7:\ntraining_loss = 0.6652', '\\n', 'Epoch 7: SES = 0.5043', '\\n', 'Epoch 8:\ntraining_loss = 0.6606', '\\n', 'Epoch 8: SES = 0.5052', '\\n', 'Epoch 9:\ntraining_loss = 0.6561', '\\n', 'Epoch 9: SES = 0.5064', '\\n', 'Epoch 10:\ntraining_loss = 0.6507', '\\n', 'Epoch 10: SES = 0.5073', '\\n', 'Training with\nlearning rate: 0.001', '\\n', 'Epoch 1: training_loss = 0.6846', '\\n', 'Epoch 1:\nSES = 0.4662', '\\n', 'Epoch 2: training_loss = 0.6346', '\\n', 'Epoch 2: SES =\n0.4839', '\\n', 'Epoch 3: training_loss = 0.5854', '\\n', 'Epoch 3: SES = 0.4979',\n'\\n', 'Epoch 4: training_loss = 0.5298', '\\n', 'Epoch 4: SES = 0.5071', '\\n',\n'Epoch 5: training_loss = 0.4684', '\\n', 'Epoch 5: SES = 0.5161', '\\n', 'Epoch\n6: training_loss = 0.4083', '\\n', 'Epoch 6: SES = 0.5193', '\\n', 'Epoch 7:\ntraining_loss = 0.3561', '\\n', 'Epoch 7: SES = 0.5196', '\\n', 'Epoch 8:\ntraining_loss = 0.3022', '\\n', 'Epoch 8: SES = 0.5192', '\\n', 'Epoch 9:\ntraining_loss = 0.2680', '\\n', 'Epoch 9: SES = 0.5180', '\\n', 'Epoch 10:\ntraining_loss = 0.2344', '\\n', 'Epoch 10: SES = 0.5182', '\\n', 'Training with\nlearning rate: 0.01', '\\n', 'Epoch 1: training_loss = 0.4798', '\\n', 'Epoch 1:\nSES = 0.4894', '\\n', 'Epoch 2: training_loss = 0.1575', '\\n', 'Epoch 2: SES =\n0.5241', '\\n', 'Epoch 3: training_loss = 0.0907', '\\n', 'Epoch 3: SES = 0.5142',\n'\\n', 'Epoch 4: training_loss = 0.0671', '\\n', 'Epoch 4: SES = 0.5212', '\\n',\n'Epoch 5: training_loss = 0.0642', '\\n', 'Epoch 5: SES = 0.5193', '\\n', 'Epoch\n6: training_loss = 0.0552', '\\n', 'Epoch 6: SES = 0.5131', '\\n', 'Epoch 7:\ntraining_loss = 0.0484', '\\n', 'Epoch 7: SES = 0.5050', '\\n', 'Epoch 8:\ntraining_loss = 0.0449', '\\n', 'Epoch 8: SES = 0.5223', '\\n', 'Epoch 9:\ntraining_loss = 0.0388', '\\n', 'Epoch 9: SES = 0.5182', '\\n', 'Epoch 10:\ntraining_loss = 0.0351', '\\n', 'Epoch 10: SES = 0.5185', '\\n', 'Execution time:\n3 seconds seconds (time limit is 10 minutes).']", "['Using device: cuda', '\\n', 'Training with batch size: 16', '\\n', 'Epoch 1:\ntraining_loss = 0.6558', '\\n', 'Epoch 1: SES = 0.5102', '\\n', 'Epoch 2:\ntraining_loss = 0.5680', '\\n', 'Epoch 2: SES = 0.5103', '\\n', 'Epoch 3:\ntraining_loss = 0.4669', '\\n', 'Epoch 3: SES = 0.5040', '\\n', 'Epoch 4:\ntraining_loss = 0.3667', '\\n', 'Epoch 4: SES = 0.5064', '\\n', 'Epoch 5:\ntraining_loss = 0.2885', '\\n', 'Epoch 5: SES = 0.5052', '\\n', 'Epoch 6:\ntraining_loss = 0.2339', '\\n', 'Epoch 6: SES = 0.5074', '\\n', 'Epoch 7:\ntraining_loss = 0.1957', '\\n', 'Epoch 7: SES = 0.5079', '\\n', 'Epoch 8:\ntraining_loss = 0.1675', '\\n', 'Epoch 8: SES = 0.5111', '\\n', 'Epoch 9:\ntraining_loss = 0.1468', '\\n', 'Epoch 9: SES = 0.5115', '\\n', 'Epoch 10:\ntraining_loss = 0.1307', '\\n', 'Epoch 10: SES = 0.5128', '\\n', 'Training with\nbatch size: 32', '\\n', 'Epoch 1: training_loss = 0.6846', '\\n', 'Epoch 1: SES =\n0.4662', '\\n', 'Epoch 2: training_loss = 0.6346', '\\n', 'Epoch 2: SES = 0.4839',\n'\\n', 'Epoch 3: training_loss = 0.5854', '\\n', 'Epoch 3: SES = 0.4979', '\\n',\n'Epoch 4: training_loss = 0.5298', '\\n', 'Epoch 4: SES = 0.5071', '\\n', 'Epoch\n5: training_loss = 0.4684', '\\n', 'Epoch 5: SES = 0.5161', '\\n', 'Epoch 6:\ntraining_loss = 0.4083', '\\n', 'Epoch 6: SES = 0.5193', '\\n', 'Epoch 7:\ntraining_loss = 0.3561', '\\n', 'Epoch 7: SES = 0.5196', '\\n', 'Epoch 8:\ntraining_loss = 0.3022', '\\n', 'Epoch 8: SES = 0.5192', '\\n', 'Epoch 9:\ntraining_loss = 0.2680', '\\n', 'Epoch 9: SES = 0.5180', '\\n', 'Epoch 10:\ntraining_loss = 0.2344', '\\n', 'Epoch 10: SES = 0.5182', '\\n', 'Training with\nbatch size: 64', '\\n', 'Epoch 1: training_loss = 0.7237', '\\n', 'Epoch 1: SES =\n0.4271', '\\n', 'Epoch 2: training_loss = 0.6860', '\\n', 'Epoch 2: SES = 0.4407',\n'\\n', 'Epoch 3: training_loss = 0.6524', '\\n', 'Epoch 3: SES = 0.4522', '\\n',\n'Epoch 4: training_loss = 0.6208', '\\n', 'Epoch 4: SES = 0.4598', '\\n', 'Epoch\n5: training_loss = 0.5881', '\\n', 'Epoch 5: SES = 0.4634', '\\n', 'Epoch 6:\ntraining_loss = 0.5564', '\\n', 'Epoch 6: SES = 0.4673', '\\n', 'Epoch 7:\ntraining_loss = 0.5208', '\\n', 'Epoch 7: SES = 0.4701', '\\n', 'Epoch 8:\ntraining_loss = 0.4850', '\\n', 'Epoch 8: SES = 0.4705', '\\n', 'Epoch 9:\ntraining_loss = 0.4482', '\\n', 'Epoch 9: SES = 0.4737', '\\n', 'Epoch 10:\ntraining_loss = 0.4146', '\\n', 'Epoch 10: SES = 0.4761', '\\n', 'Execution time:\n3 seconds seconds (time limit is 10 minutes).']", "['Using device: cuda', '\\n', 'Training with learning rate: 0.0001', '\\n', 'Epoch\n1: training_loss = 0.6582', '\\n', 'Epoch 1: SES = 0.5274', '\\n', 'Epoch 2:\ntraining_loss = 0.6526', '\\n', 'Epoch 2: SES = 0.5253', '\\n', 'Epoch 3:\ntraining_loss = 0.6475', '\\n', 'Epoch 3: SES = 0.5233', '\\n', 'Epoch 4:\ntraining_loss = 0.6441', '\\n', 'Epoch 4: SES = 0.5218', '\\n', 'Epoch 5:\ntraining_loss = 0.6369', '\\n', 'Epoch 5: SES = 0.5193', '\\n', 'Epoch 6:\ntraining_loss = 0.6315', '\\n', 'Epoch 6: SES = 0.5173', '\\n', 'Epoch 7:\ntraining_loss = 0.6269', '\\n', 'Epoch 7: SES = 0.5153', '\\n', 'Epoch 8:\ntraining_loss = 0.6239', '\\n', 'Epoch 8: SES = 0.5134', '\\n', 'Epoch 9:\ntraining_loss = 0.6185', '\\n', 'Epoch 9: SES = 0.5120', '\\n', 'Epoch 10:\ntraining_loss = 0.6135', '\\n', 'Epoch 10: SES = 0.5101', '\\n', 'Training with\nlearning rate: 0.001', '\\n', 'Epoch 1: training_loss = 0.6828', '\\n', 'Epoch 1:\nSES = 0.5032', '\\n', 'Epoch 2: training_loss = 0.6266', '\\n', 'Epoch 2: SES =\n0.5003', '\\n', 'Epoch 3: training_loss = 0.5696', '\\n', 'Epoch 3: SES = 0.4959',\n'\\n', 'Epoch 4: training_loss = 0.5109', '\\n', 'Epoch 4: SES = 0.4958', '\\n',\n'Epoch 5: training_loss = 0.4519', '\\n', 'Epoch 5: SES = 0.4919', '\\n', 'Epoch\n6: training_loss = 0.3943', '\\n', 'Epoch 6: SES = 0.4964', '\\n', 'Epoch 7:\ntraining_loss = 0.3398', '\\n', 'Epoch 7: SES = 0.4978', '\\n', 'Epoch 8:\ntraining_loss = 0.2944', '\\n', 'Epoch 8: SES = 0.4978', '\\n', 'Epoch 9:\ntraining_loss = 0.2574', '\\n', 'Epoch 9: SES = 0.4977', '\\n', 'Epoch 10:\ntraining_loss = 0.2274', '\\n', 'Epoch 10: SES = 0.4994', '\\n', 'Training with\nlearning rate: 0.01', '\\n', 'Epoch 1: training_loss = 0.4892', '\\n', 'Epoch 1:\nSES = 0.4950', '\\n', 'Epoch 2: training_loss = 0.1561', '\\n', 'Epoch 2: SES =\n0.4898', '\\n', 'Epoch 3: training_loss = 0.0801', '\\n', 'Epoch 3: SES = 0.4938',\n'\\n', 'Epoch 4: training_loss = 0.0560', '\\n', 'Epoch 4: SES = 0.4983', '\\n',\n'Epoch 5: training_loss = 0.0441', '\\n', 'Epoch 5: SES = 0.4900', '\\n', 'Epoch\n6: training_loss = 0.0389', '\\n', 'Epoch 6: SES = 0.4922', '\\n', 'Epoch 7:\ntraining_loss = 0.0370', '\\n', 'Epoch 7: SES = 0.4888', '\\n', 'Epoch 8:\ntraining_loss = 0.0296', '\\n', 'Epoch 8: SES = 0.4914', '\\n', 'Epoch 9:\ntraining_loss = 0.0257', '\\n', 'Epoch 9: SES = 0.4936', '\\n', 'Epoch 10:\ntraining_loss = 0.0241', '\\n', 'Epoch 10: SES = 0.4900', '\\n', 'Execution time:\n3 seconds seconds (time limit is 10 minutes).']"], "analysis": ["", "", "", ""], "exc_type": [null, null, null, null], "exc_info": [null, null, null, null], "exc_stack": [null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss of the model", "data": [{"dataset_name": "synthetic_dataset", "final_value": 0.2234, "best_value": 0.2234}]}, {"metric_name": "train SES", "lower_is_better": false, "description": "Final training SES of the model", "data": [{"dataset_name": "synthetic_dataset", "final_value": 0.506, "best_value": 0.506}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss value", "data": [{"dataset_name": "synthetic_dataset", "final_value": 0.0351, "best_value": 0.0351}]}, {"metric_name": "training SES", "lower_is_better": false, "description": "Final training SES value", "data": [{"dataset_name": "synthetic_dataset", "final_value": 0.5185, "best_value": 0.5185}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss after training", "data": [{"dataset_name": "batch size = 16", "final_value": 0.1307, "best_value": 0.1307}, {"dataset_name": "batch size = 32", "final_value": 0.2344, "best_value": 0.2344}, {"dataset_name": "batch size = 64", "final_value": 0.4146, "best_value": 0.4146}]}, {"metric_name": "SES", "lower_is_better": false, "description": "Final SES after training", "data": [{"dataset_name": "batch size = 16", "final_value": 0.5128, "best_value": 0.5128}, {"dataset_name": "batch size = 32", "final_value": 0.5182, "best_value": 0.5182}, {"dataset_name": "batch size = 64", "final_value": 0.4761, "best_value": 0.4761}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss value", "data": [{"dataset_name": "synthetic_dataset", "final_value": 0.0241, "best_value": 0.0241}]}, {"metric_name": "training SES", "lower_is_better": false, "description": "Final training SES value", "data": [{"dataset_name": "synthetic_dataset", "final_value": 0.49, "best_value": 0.49}]}]}], "is_best_node": [false, true, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_614499748ba74239895df856d3ca7f46_proc_4412/synthetic_dataset_training_loss.png", "../../logs/0-run/experiment_results/experiment_614499748ba74239895df856d3ca7f46_proc_4412/synthetic_dataset_ses.png"], ["../../logs/0-run/experiment_results/experiment_74800e3e567a48abaff37837f86095a4_proc_5019/synthetic_dataset_training_loss.png", "../../logs/0-run/experiment_results/experiment_74800e3e567a48abaff37837f86095a4_proc_5019/synthetic_dataset_ses.png"], ["../../logs/0-run/experiment_results/experiment_d0d31406b9504c0a84d36cd3e8911215_proc_5019/training_loss_batch_16.png", "../../logs/0-run/experiment_results/experiment_d0d31406b9504c0a84d36cd3e8911215_proc_5019/training_loss_batch_32.png", "../../logs/0-run/experiment_results/experiment_d0d31406b9504c0a84d36cd3e8911215_proc_5019/training_loss_batch_64.png", "../../logs/0-run/experiment_results/experiment_d0d31406b9504c0a84d36cd3e8911215_proc_5019/ses_batch_16.png", "../../logs/0-run/experiment_results/experiment_d0d31406b9504c0a84d36cd3e8911215_proc_5019/ses_batch_32.png", "../../logs/0-run/experiment_results/experiment_d0d31406b9504c0a84d36cd3e8911215_proc_5019/ses_batch_64.png"], ["../../logs/0-run/experiment_results/experiment_a92234705f4045029221804be69cab29_proc_5019/synthetic_dataset_training_loss.png", "../../logs/0-run/experiment_results/experiment_a92234705f4045029221804be69cab29_proc_5019/synthetic_dataset_ses.png"]], "plot_paths": [["experiments/2025-11-27_04-36-32_scenario_simulation_decision_making_attempt_0/logs/0-run/experiment_results/experiment_614499748ba74239895df856d3ca7f46_proc_4412/synthetic_dataset_training_loss.png", "experiments/2025-11-27_04-36-32_scenario_simulation_decision_making_attempt_0/logs/0-run/experiment_results/experiment_614499748ba74239895df856d3ca7f46_proc_4412/synthetic_dataset_ses.png"], ["experiments/2025-11-27_04-36-32_scenario_simulation_decision_making_attempt_0/logs/0-run/experiment_results/experiment_74800e3e567a48abaff37837f86095a4_proc_5019/synthetic_dataset_training_loss.png", "experiments/2025-11-27_04-36-32_scenario_simulation_decision_making_attempt_0/logs/0-run/experiment_results/experiment_74800e3e567a48abaff37837f86095a4_proc_5019/synthetic_dataset_ses.png"], ["experiments/2025-11-27_04-36-32_scenario_simulation_decision_making_attempt_0/logs/0-run/experiment_results/experiment_d0d31406b9504c0a84d36cd3e8911215_proc_5019/training_loss_batch_16.png", "experiments/2025-11-27_04-36-32_scenario_simulation_decision_making_attempt_0/logs/0-run/experiment_results/experiment_d0d31406b9504c0a84d36cd3e8911215_proc_5019/training_loss_batch_32.png", "experiments/2025-11-27_04-36-32_scenario_simulation_decision_making_attempt_0/logs/0-run/experiment_results/experiment_d0d31406b9504c0a84d36cd3e8911215_proc_5019/training_loss_batch_64.png", "experiments/2025-11-27_04-36-32_scenario_simulation_decision_making_attempt_0/logs/0-run/experiment_results/experiment_d0d31406b9504c0a84d36cd3e8911215_proc_5019/ses_batch_16.png", "experiments/2025-11-27_04-36-32_scenario_simulation_decision_making_attempt_0/logs/0-run/experiment_results/experiment_d0d31406b9504c0a84d36cd3e8911215_proc_5019/ses_batch_32.png", "experiments/2025-11-27_04-36-32_scenario_simulation_decision_making_attempt_0/logs/0-run/experiment_results/experiment_d0d31406b9504c0a84d36cd3e8911215_proc_5019/ses_batch_64.png"], ["experiments/2025-11-27_04-36-32_scenario_simulation_decision_making_attempt_0/logs/0-run/experiment_results/experiment_a92234705f4045029221804be69cab29_proc_5019/synthetic_dataset_training_loss.png", "experiments/2025-11-27_04-36-32_scenario_simulation_decision_making_attempt_0/logs/0-run/experiment_results/experiment_a92234705f4045029221804be69cab29_proc_5019/synthetic_dataset_ses.png"]], "plot_analyses": [[{"analysis": "The Scenario Evaluation Score (SES) exhibits fluctuations over the epochs, peaking around the second epoch before declining and stabilizing towards the end. This variability suggests that while the model is capable of evaluating scenarios, there may be instability in its performance during training. The initial increase in SES indicates that the model is learning to evaluate scenarios better at first, but the subsequent decline raises concerns about its robustness. Further analysis and possibly adjustments in training or architecture might be necessary to improve stability and ensure consistent performance.", "plot_path": "experiments/2025-11-27_04-36-32_scenario_simulation_decision_making_attempt_0/logs/0-run/experiment_results/experiment_614499748ba74239895df856d3ca7f46_proc_4412/synthetic_dataset_training_loss.png"}], [{"analysis": "The SES plot shows fluctuations in the Scenario Evaluation Score over the epochs. While there is an initial increase in the SES value, it stabilizes around 0.50 after several epochs, suggesting that the model's performance in evaluating scenarios is relatively consistent but not significantly improving. This could indicate that while the model is learning well in terms of loss, the evaluation of scenarios might require further tuning or adjustments in the dataset or hyperparameters.", "plot_path": "experiments/2025-11-27_04-36-32_scenario_simulation_decision_making_attempt_0/logs/0-run/experiment_results/experiment_74800e3e567a48abaff37837f86095a4_proc_5019/synthetic_dataset_training_loss.png"}], [{"analysis": "The SES for batch size 64 shows a gradual increase but remains lower than that for batch sizes 16 and 32, peaking at about 0.47. This indicates that the larger batch size may hinder the model's ability to learn effectively, as evidenced by the lower evaluation scores compared to the smaller batch sizes.", "plot_path": "experiments/2025-11-27_04-36-32_scenario_simulation_decision_making_attempt_0/logs/0-run/experiment_results/experiment_d0d31406b9504c0a84d36cd3e8911215_proc_5019/training_loss_batch_16.png"}], [{"analysis": "The training loss shows a general downward trend over the epochs, indicating that the model is learning effectively. Initially, the loss decreases rapidly, suggesting that the model is quickly adapting to the training data. After around 20 epochs, the loss stabilizes and approaches a minimum value, indicating that the model may have reached a point of convergence. This is a positive sign, as it suggests that the hyperparameter tuning may be yielding beneficial results without overfitting, at least up to this point in training.", "plot_path": "experiments/2025-11-27_04-36-32_scenario_simulation_decision_making_attempt_0/logs/0-run/experiment_results/experiment_a92234705f4045029221804be69cab29_proc_5019/synthetic_dataset_training_loss.png"}, {"analysis": "The Scenario Evaluation Score (SES) also demonstrates a downward trend over the epochs, which may indicate that the model is becoming more selective or refined in its evaluation of scenarios. However, the SES value appears to plateau towards the end of the training, suggesting that there might be diminishing returns in evaluating scenarios as training progresses. This could indicate a need for further tuning or the introduction of additional datasets to provide more diverse scenarios for evaluation.", "plot_path": "experiments/2025-11-27_04-36-32_scenario_simulation_decision_making_attempt_0/logs/0-run/experiment_results/experiment_a92234705f4045029221804be69cab29_proc_5019/synthetic_dataset_ses.png"}]], "vlm_feedback_summary": ["The training loss indicates successful learning, while the SES shows variability\nthat may need further investigation.", "The training loss indicates effective learning, while the SES plot suggests\nstable but potentially limited improvement in scenario evaluation.", "The analysis indicates that smaller batch sizes (16 and 32) are more effective\nin training the model, leading to lower training losses and higher scenario\nevaluation scores compared to the larger batch size (64). Adjusting batch sizes\nmay enhance the learning process.", "Both the training loss and SES trends indicate that the model is learning and\nadapting, but further improvements may be necessary to enhance scenario\nevaluation effectiveness."], "exec_time": [6.819814682006836, 3.295149803161621, 3.5008907318115234, 3.338761329650879], "exec_time_feedback": ["", "", "", ""], "datasets_successfully_tested": [["synthetic_dataset"], ["synthetic_dataset"], ["[16", "32]"], ["synthetic_dataset"]], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# Define working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot training loss\ntry:\n    plt.figure()\n    plt.plot(\n        experiment_data[\"synthetic_dataset\"][\"losses\"][\"train\"], label=\"Training Loss\"\n    )\n    plt.title(\"Training Loss Over Epochs\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_dataset_training_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating training loss plot: {e}\")\n    plt.close()\n\n# Plot SES\ntry:\n    plt.figure()\n    plt.plot(\n        experiment_data[\"synthetic_dataset\"][\"metrics\"][\"train\"],\n        label=\"Scenario Evaluation Score (SES)\",\n    )\n    plt.title(\"SES Over Epochs\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"SES\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_dataset_ses.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating SES plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\ntry:\n    plt.figure()\n    train_losses = experiment_data[\"hyperparam_tuning_learning_rate\"][\n        \"synthetic_dataset\"\n    ][\"losses\"][\"train\"]\n    plt.plot(train_losses, label=\"Training Loss\")\n    plt.title(\"Training Loss over Epochs\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_dataset_training_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating training loss plot: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    train_metrics = experiment_data[\"hyperparam_tuning_learning_rate\"][\n        \"synthetic_dataset\"\n    ][\"metrics\"][\"train\"]\n    plt.plot(train_metrics, label=\"Scenario Evaluation Score (SES)\", color=\"orange\")\n    plt.title(\"SES over Epochs\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"SES Value\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_dataset_ses.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating SES plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_data = np.load(\n    os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n).item()\n\n# Plotting training loss\nfor batch_size, data in experiment_data[\"batch_size_tuning\"].items():\n    try:\n        plt.figure()\n        plt.plot(data[\"losses\"][\"train\"], label=\"Training Loss\")\n        plt.title(f\"Training Loss for Batch Size {batch_size}\")\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"training_loss_batch_{batch_size}.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for batch size {batch_size}: {e}\")\n        plt.close()\n\n# Plotting Scenario Evaluation Score\nfor batch_size, data in experiment_data[\"batch_size_tuning\"].items():\n    try:\n        plt.figure()\n        plt.plot(data[\"metrics\"][\"train\"], label=\"SES\")\n        plt.title(f\"Scenario Evaluation Score for Batch Size {batch_size}\")\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"SES\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"ses_batch_{batch_size}.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating SES plot for batch size {batch_size}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\ntry:\n    plt.figure()\n    train_losses = experiment_data[\"hyperparam_tuning_learning_rate\"][\n        \"synthetic_dataset\"\n    ][\"losses\"][\"train\"]\n    plt.plot(train_losses, label=\"Training Loss\")\n    plt.title(\"Training Loss over Epochs\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_dataset_training_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating training loss plot: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    train_metrics = experiment_data[\"hyperparam_tuning_learning_rate\"][\n        \"synthetic_dataset\"\n    ][\"metrics\"][\"train\"]\n    plt.plot(train_metrics, label=\"Scenario Evaluation Score (SES)\", color=\"orange\")\n    plt.title(\"SES over Epochs\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"SES Value\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_dataset_ses.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating SES plot: {e}\")\n    plt.close()\n"], "plot_plan": ["I will create visualizations for the training loss and the Scenario Evaluation Score (SES) derived from the experiment data saved in `experiment_data.npy`. The first plot will display the training loss across epochs, providing insight into the model's performance during training. The second plot will visualize the SES, which indicates the model's outputs relative to the ground truth, also across epochs. Each plot will have a clear title and subtitle, and they will be saved in the working directory.\n\nThe code will handle exceptions for each plot to ensure figures are closed properly regardless of success. I will ensure that descriptive names are used for the saved plot files, containing details about the dataset and type of plot.", "To visualize the experiment results stored in `experiment_data.npy`, I will first load the data and extract the relevant metrics for plotting. The code will create separate plots for training losses and the Scenario Evaluation Score (SES) over epochs. Each plot will be saved in the `working_dir`, and I'll use try-except blocks to handle any potential errors during the plotting process. The title and subtitles for each plot will be descriptive to ensure clarity regarding the datasets and types of plots. I'll selectively plot training and validation metrics, ensuring not to overload with too many similar figures.", "To visualize the training and evaluation results from the experiment data stored in `experiment_data.npy`, I'll create several plots that highlight different metrics. First, I'll plot the training loss over epochs for each batch size, using a line graph with appropriate titles and subtitles. Next, I will create a plot for the Scenario Evaluation Score (SES) across epochs for each batch size. If multiple epochs are involved, I will select only a subset of them to avoid overcrowding the visuals, ensuring that we have at most 5 plots per type. Each plot will be saved to the `working_dir` with descriptive filenames indicating what they represent. I will handle any exceptions that arise during the plotting process, ensuring that figures are closed properly regardless of errors.", null], "ablation_name": [null, null, null, null], "hyperparam_name": [null, "learning_rate", "batch_size", null], "is_seed_node": [false, false, false, true], "is_seed_agg_node": [false, false, false, false], "parse_metrics_plan": ["I will load the `experiment_data.npy` file from the designated working directory\nand extract the relevant metrics. I'll print the name of the dataset and the\nfinal values for the training loss and the Scenario Evaluation Score (SES)\nclearly labeled. This will ensure appropriate clarity in the results displayed.", "To analyze the metrics from the `experiment_data.npy`, I will load the file and\nextract the relevant metrics for the synthetic dataset. I will ensure to print\nthe name of the dataset along with the metric names such as training loss and\ntraining accuracy, while only displaying the final values for clarity. The code\nwill follow the specified structure and will execute upon running.", "To analyze the metrics from the `experiment_data.npy` file, we will start by\nloading the data from the specified working directory. After that, we will\niterate through the metrics for each batch size tuning dataset, printing the\naverage training loss and Scenario Evaluation Score (SES) for each\nconfiguration. This will ensure that we clearly specify the name of each metric\nalongside its corresponding value.", "To analyze the metrics from the `experiment_data.npy`, I will load the file and\nextract the relevant metrics for the synthetic dataset. I will ensure to print\nthe name of the dataset along with the metric names such as training loss and\ntraining accuracy, while only displaying the final values for clarity. The code\nwill follow the specified structure and will execute upon running."], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Load experiment data\nexperiment_data = np.load(\n    os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\"), allow_pickle=True\n).item()\n\n# Extract metrics for each dataset\nfor dataset_name, data in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n    train_loss = data[\"losses\"][\"train\"][-1] if data[\"losses\"][\"train\"] else None\n    train_ses = data[\"metrics\"][\"train\"][-1] if data[\"metrics\"][\"train\"] else None\n\n    print(\n        f\"Final training loss: {train_loss:.4f}\"\n        if train_loss is not None\n        else \"No training loss data available.\"\n    )\n    print(\n        f\"Final train SES: {train_ses:.4f}\"\n        if train_ses is not None\n        else \"No training SES data available.\"\n    )\n", "import os\nimport numpy as np\n\n# Load experiment data\nexperiment_data = np.load(\n    os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\"), allow_pickle=True\n).item()\n\n# Extract and print metrics\ndataset_name = \"synthetic_dataset\"\nprint(f\"Dataset: {dataset_name}\")\n\ntrain_loss = experiment_data[\"hyperparam_tuning_learning_rate\"][dataset_name][\"losses\"][\n    \"train\"\n][-1]\ntrain_ses = experiment_data[\"hyperparam_tuning_learning_rate\"][dataset_name][\"metrics\"][\n    \"train\"\n][-1]\n\nprint(f\"Final training loss: {train_loss:.4f}\")\nprint(f\"Final training SES: {train_ses:.4f}\")\n", "import os\nimport numpy as np\n\n# Load experiment data\nexperiment_data = np.load(\n    os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\"), allow_pickle=True\n).item()\n\n# Iterate through each batch size and extract metrics\nfor batch_size, data in experiment_data[\"batch_size_tuning\"].items():\n    print(f\"Dataset: batch size = {batch_size}\")\n\n    # Get the final training loss\n    final_training_loss = data[\"losses\"][\"train\"][-1]\n    print(f\"Final training loss: {final_training_loss:.4f}\")\n\n    # Get the final Scenario Evaluation Score (SES)\n    final_ses = data[\"metrics\"][\"train\"][-1]\n    print(f\"Final SES: {final_ses:.4f}\")\n", "import os\nimport numpy as np\n\n# Load experiment data\nexperiment_data = np.load(\n    os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\"), allow_pickle=True\n).item()\n\n# Extract and print metrics\ndataset_name = \"synthetic_dataset\"\nprint(f\"Dataset: {dataset_name}\")\n\ntrain_loss = experiment_data[\"hyperparam_tuning_learning_rate\"][dataset_name][\"losses\"][\n    \"train\"\n][-1]\ntrain_ses = experiment_data[\"hyperparam_tuning_learning_rate\"][dataset_name][\"metrics\"][\n    \"train\"\n][-1]\n\nprint(f\"Final training loss: {train_loss:.4f}\")\nprint(f\"Final training SES: {train_ses:.4f}\")\n"], "parse_term_out": ["['Dataset: synthetic_dataset', '\\n', 'Final training loss: 0.2234', '\\n', 'Final\ntrain SES: 0.5060', '\\n', 'Execution time: a moment seconds (time limit is 10\nminutes).']", "['Dataset: synthetic_dataset', '\\n', 'Final training loss: 0.0351', '\\n', 'Final\ntraining SES: 0.5185', '\\n', 'Execution time: a moment seconds (time limit is 10\nminutes).']", "['Dataset: batch size = 16', '\\n', 'Final training loss: 0.1307', '\\n', 'Final\nSES: 0.5128', '\\n', 'Dataset: batch size = 32', '\\n', 'Final training loss:\n0.2344', '\\n', 'Final SES: 0.5182', '\\n', 'Dataset: batch size = 64', '\\n',\n'Final training loss: 0.4146', '\\n', 'Final SES: 0.4761', '\\n', 'Execution time:\na moment seconds (time limit is 10 minutes).']", "['Dataset: synthetic_dataset', '\\n', 'Final training loss: 0.0241', '\\n', 'Final\ntraining SES: 0.4900', '\\n', 'Execution time: a moment seconds (time limit is 10\nminutes).']"], "parse_exc_type": [null, null, null, null], "parse_exc_info": [null, null, null, null], "parse_exc_stack": [null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
